It is a difficult task for visually impaired people to see what is around them and
recognize it. Basic street signs or the surrounding environment are almost always
difficult for them to comprehend. This model proposes a mechanism that can assist
visually impaired people in better comprehending their surroundings. Image captioning
is the process of generating a textual description of an image. It uses both natural
language processing and processing of images and videos to generate captions. To make
life convenient , the implementation of visual aid techniques using Video Captioning
and Optical Character Recognition (OCR) is performed. The requirements include a
camera and a GPU incorporated system.
Here we have a Video Captioning module that processes the environment and
generates suitable descriptions or captions, an OCR module for recognizing text on
notice boards, documents, and a Text-To-Speech (TTS) module. The OCR module
helps in detecting the text on notice or signboards in the immediate region, while the
text to speech module is typically used to convert data into a format that is accessible
to visually impaired people, such as voice notifications (audio output). This makes it
easier for visually challenged people to be aware of what is present in their immediate
vicinity. The model produces audio in different languages such as English, Hindi, and
Telugu, allowing it to adapt to the needs of the vernacular community.
